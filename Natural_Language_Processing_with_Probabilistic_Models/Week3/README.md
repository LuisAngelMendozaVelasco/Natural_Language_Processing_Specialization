# Autocomplete and Language Models

Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter!

## Learning Objectives

- Conditional probabilities
- Text pre-processing
- Language modeling
- Perplexity
- K-smoothing
- N-grams
- Backoff
- Tokenization

## Lecture: Autocomplete

- [Video - Week Introduction](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/8UOFz/week-introduction)

- [Video - N-Grams: Overview](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/CzdJQ/n-grams-overview)

- [Reading - N-Grams Overview](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/amsS9/n-grams-overview)

- [Video - N-grams and Probabilities](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/i8pZr/n-grams-and-probabilities)

- [Reading - N-grams and Probabilities](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/fhJRU/n-grams-and-probabilities)

- [Video -Sequence Probabilities](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/czjtB/sequence-probabilities)

- [Reading - Sequence Probabilities](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/bfpVd/sequence-probabilities)

- [Video - Starting and Ending Sentences](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/fWTAg/starting-and-ending-sentences)

- [Reading - Starting and Ending Sentences](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/7QB8H/starting-and-ending-sentences)

- [Lab - Lecture notebook: Corpus preprocessing for N-grams](./Labs/C2_W3_lecture_nb_01_corpus_preprocessing.ipynb)

- [Video - The N-gram Language Model](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/NJfFy/the-n-gram-language-model)

- [Reading - The N-gram Language Model](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/6UwRY/the-n-gram-language-model)

- [Video - Language Model Evaluation](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/SEO4T/language-model-evaluation)

- [Lab - Lecture notebook: Building the language model](./Labs/C2_W3_lecture_nb_02_building_the_language_model.ipynb)

- [Reading - Language Model Evaluation](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/71fL4/language-model-evaluation)

- [Video - Out of Vocabulary Words](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/ggG3p/out-of-vocabulary-words)

- [Reading - Out of Vocabulary Words](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/EAiBM/out-of-vocabulary-words)

- [Video - Smoothing](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/TGL4W/smoothing)

- [Reading - Smoothing](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/WivYa/smoothing)

- [Lab - Lecture notebook: Language model generalization](./Labs/C2_W3_lecture_nb_03_oov.ipynb)

- [Video - Week Summary](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/z0uT2/week-summary)

- [Reading - Week Summary](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/3o0kq/week-summary)

- [Video - Week Conclusion](https://www.coursera.org/learn/probabilistic-models-in-nlp/lecture/kfqJj/week-conclusion)

## Lecture Notes (Optional)

- [Reading - Lecture Notes W3](./Readings/C2_W3.pdf)

## Assignment: Autocomplete

- [Lab - Autocomplete](./Labs/C2_W3_Assignment.ipynb)

# Neural Machine Translation

Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then build a Neural Machine Translation model with Attention that translates English sentences into German.

## Learning Objectives

- Explain how an Encoder/Decoder model works
- Apply word alignment for machine translation
- Train a Neural Machine Translation model with Attention
- Develop intuition for how teacher forcing helps a translation model check its predictions
- Use BLEU score and ROUGE score to evaluate machine-generated text quality
- Describe several decoding methods including MBR and Beam search

## Neural Machine Translation

- [Video - Course 4 Introduction](https://www.coursera.org/learn/attention-models-in-nlp/lecture/EXHcS/course-4-introduction)

- [Video - Week Introduction](https://www.coursera.org/learn/attention-models-in-nlp/lecture/GHweQ/week-introduction)

- [Video - Seq2seq](https://www.coursera.org/learn/attention-models-in-nlp/lecture/VhWLB/seq2seq)

- [Video - Seq2seq Model with Attention](https://www.coursera.org/learn/attention-models-in-nlp/lecture/LLvlj/seq2seq-model-with-attention)

- [Lab - Ungraded Lab: Basic Attention](./Labs/C4W1_Basic_Attention.ipynb)

- [Reading - Background on seq2seq](https://www.coursera.org/learn/attention-models-in-nlp/supplement/HJ84q/background-on-seq2seq)

- [Video - Queries, Keys, Values, and Attention](https://www.coursera.org/learn/attention-models-in-nlp/lecture/hPxD1/queries-keys-values-and-attention)

- [Lab - Ungraded Lab: Scaled Dot-Product Attention](./Labs/C4W1_QKV_Attention.ipynb)

- [Video - Setup for Machine Translation](https://www.coursera.org/learn/attention-models-in-nlp/lecture/87aPC/setup-for-machine-translation)

- [Video - Teacher Forcing](https://www.coursera.org/learn/attention-models-in-nlp/lecture/noMUB/teacher-forcing)

- [Video - NMT Model with Attention](https://www.coursera.org/learn/attention-models-in-nlp/lecture/CieMg/nmt-model-with-attention)

- [Video - BLEU Score](https://www.coursera.org/learn/attention-models-in-nlp/lecture/4ZdLf/bleu-score)

- [Lab - Ungraded Lab: BLEU Score](./Labs/C4W1_Bleu_Score.ipynb)

- [Video - ROUGE-N Score](https://www.coursera.org/learn/attention-models-in-nlp/lecture/CtY2v/rouge-n-score)

- [Video - Sampling and Decoding](https://www.coursera.org/learn/attention-models-in-nlp/lecture/5OVYd/sampling-and-decoding)

- [Video - Beam Search](https://www.coursera.org/learn/attention-models-in-nlp/lecture/Ukk3c/beam-search)

- [Video - Minimum Bayes Risk](https://www.coursera.org/learn/attention-models-in-nlp/lecture/2UOc2/minimum-bayes-risk)

- [Video - Week Conclusion](https://www.coursera.org/learn/attention-models-in-nlp/lecture/ggQCj/week-conclusion)

- [Reading - Content Resource](https://www.coursera.org/learn/attention-models-in-nlp/supplement/IHbrg/links-to-the-resources)

## Lecture Notes (Notes)

- [Reading - Lecture Notes W1](./Readings/C4_W1.pdf)

## Assignment

- [Lab - NMT with Attention (Tensorflow)](./Labs/C4W1_Assignment.ipynb)

## Heroes of NLP: Oren Etzioni

- [Video - Andrew Ng with Oren Etzioni](https://www.coursera.org/learn/attention-models-in-nlp/lecture/n9npP/andrew-ng-with-oren-etzioni)

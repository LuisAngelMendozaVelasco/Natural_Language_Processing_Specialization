# Text Summarization

Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.

## Learning Objectives

- Describe the three basic types of attention
- Name the two types of layers in a Transformer
- Define three main matrices in attention
- Interpret the math behind scaled dot product attention, causal attention, and multi-head attention
- Use articles and their summaries to create input features for training a text summarizer
- Build a Transformer decoder model (GPT-2)

## Text Summarization

- [Video - Week Introduction](https://www.coursera.org/learn/attention-models-in-nlp/lecture/R1600/week-introduction)

- [Video - Transformers vs RNNs](https://www.coursera.org/learn/attention-models-in-nlp/lecture/glNgT/transformers-vs-rnns)

- [Reading - Transformers vs RNNs](https://www.coursera.org/learn/attention-models-in-nlp/supplement/CtX3o/transformers-vs-rnns)

- [Video - Transformers overview](https://www.coursera.org/learn/attention-models-in-nlp/lecture/HKf9J/transformers-overview)

- [Video - Transformer Applications](https://www.coursera.org/learn/attention-models-in-nlp/lecture/B655z/transformer-applications)

- [Reading - Transformer Applications](https://www.coursera.org/learn/attention-models-in-nlp/supplement/ZQife/transformer-applications)

- [Video - Scaled and Dot-Product Attention](https://www.coursera.org/learn/attention-models-in-nlp/lecture/lO5yN/scaled-and-dot-product-attention)

- [Video - Masked Self Attention](https://www.coursera.org/learn/attention-models-in-nlp/lecture/AMz8y/masked-self-attention)

- [Video - Multi-head Attention](https://www.coursera.org/learn/attention-models-in-nlp/lecture/K5zR3/multi-head-attention)

- [Reading - Multi-head Attention](https://www.coursera.org/learn/attention-models-in-nlp/supplement/5a5gB/multi-head-attention)

- [Lab - Attention](./Labs/C4W2_Attention.ipynb)

- [Lab - Masking](./Labs/C4W2_Masking.ipynb)

- [Lab - Positional encoding](./Labs/C4W2_Positional_Encodincg.ipynb)

- [Video - Transformer Decoder](https://www.coursera.org/learn/attention-models-in-nlp/lecture/rDLol/transformer-decoder)

- [Reading - Transformer Decoder](https://www.coursera.org/learn/attention-models-in-nlp/supplement/pn2zX/transformer-decoder)

- [Video - Transformer Summarizer](https://www.coursera.org/learn/attention-models-in-nlp/lecture/ZhWeC/transformer-summarizer)

- [Video - Week Conclusion](https://www.coursera.org/learn/attention-models-in-nlp/lecture/5j8JE/week-conclusion)

- [Reading - Content Resource](https://www.coursera.org/learn/attention-models-in-nlp/supplement/IHbrg/links-to-the-resources)

## Lecture Notes (Optional)

- [Reading - Lecture Notes W2](./Readings/C4_W2.pdf)

## Assignment

- [Lab - Transformer Summarizer](./Labs/C4W2_Assignment.ipynb)
